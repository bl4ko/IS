{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature Inspired Computing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe the main components of an evolutionary program: population representation, generation, selection, combination replacement and stopping criteria?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Population repesentation`: Population consists of multiple **individuals**. Each individual representing a candidate solution (represented **array of bits**). the way in which the potential solutions to the program are **represented** in the program. This could be in the form of: binary strings, lists of numbers, different data structures...\n",
    "- `Generation`: **evolution** usually starts from a population of randomly generated **individuals**, and is and **iterative process** with the population in each iteration called a **generation**. In each generation the **fitness** of every individual in the population is evaluted; the fitness is usually the value of the objective function in the optimization problem being solved. The more **fit** individuals are stochastically selected from the current population, and each individual's genome is modified (crossover/mutation) to form a new generation\n",
    "- `Selection`: During each successive generation, a portion of existing population is selected to breed a new generation. Individual solutions are selected through a **fitness-based** process, where **fitter** solutions (as measured by a fitness function) are typically more likely to be selected.\n",
    "- `Stopping criteria`:  Commonly the algorithm terminates when either a **maximum** number of generations has been produced, or a **satisfactory** fitness level has been reached for the population."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe whento use genetic algorithms?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAs are good:\n",
    "- when there is a clear way to evaluate **fitness** of a solution (and we don't know the original function - if we would you do't need GA for it)\n",
    "- when we have **big space** to search (impossible to ocnsider all possible solutions) and when we can find a good represantation of genes (agents)\n",
    "\n",
    "For example `knapsack` problem is good for GA, because we can easily evaluate fitness of a solution (sum of values of items in knapsack) and we can easily represent it (array of 0s and 1s)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Describe the strengths and weaknesses of evolutionary programs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Strengths`: robust, adaptable and general, requires only fitness function and representation of genes (easy to implement)\n",
    "\n",
    "`Weaknesses`: can get stuck in local extreme (not **complete** algorithm), can take a long time to converge to solution, time complexity rises fast with bigger population (computaly expensive)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Describe the main characterstics of genetic algorithms (GA) and genetic programming (GP)\n",
    "\n",
    "GP and GA are both types of evolutionary programming:\n",
    "\n",
    "Genetic algorithms:\n",
    "- they use a population of solutions that are encoded as strings of characters or numbers\n",
    "- they use reproduction, mutation, and selection to generate new solutions from existing population (**fixed length crossover**)\n",
    "- they use a measure of fitness to evaluate the quality of each solution and guide the search process\n",
    "- they can be used to solve a wide range of problems, including optimization, machine learning, and combinatorial optimization\n",
    "\n",
    "Genetic programming:\n",
    "- they use a population of solutions that are encoded as **tree** structures\n",
    "- crossover: exchange subtree, mutation: random change in trees (**tree length variable crossover**)\n",
    "- Variable length encoding, more flexibllle, often grow in complexity\n",
    "\n",
    "![gaprogramming](./images/gaprogramming.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe terms from evolutionary computation such as population variablitiy, fitness function, coevolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fitness function`: function that takes a solution (individual) as a input and evaluates it, to see how \"good\" the solution is.\n",
    "\n",
    "`Population variability`: we need to have a population that encompasses as big a solution space as possible to find a solution \n",
    "close to the optimal possible (e.g. $2^30$ solution space, population of 10 will probably not find a very good solution)\n",
    "\n",
    "`Coevolution`: basically crossover $\\rightarrow$ two agents affect evolution by combining traits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Describe the different gene representations in GA, operations on them and their strengths and weaknesses: bit and numeric vectors, strings permutations, trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `bit/numeric`: good for problems that can be represented with numbers, cannot represent very complex problems (e.g. good for knapsack problem)\n",
    "- `stirng permutations`: good for problems where we are looking for a solution of a sequence of numbers (TSP), then we can use GA to \"learn\" the best permutations\n",
    "- `trees`: good for problems where we want to find the formula for the solution (as formulas can be nicely represented with trees)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What are linear crossover, gray coding of binary numbers, adaptive crossover, gaussian mutation, lamarckian mutation and elitism? What are their advantages compared to baselines?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear crossover`: a technique to create new solutions by combining pieces of existing solutions. It involves a single point in the solution representation and then exchanging all the bits after that point between the two solutions. (could be multiple points). New solution is called **offsping**\n",
    "\n",
    "![linear crossover](./images/linear-crossover.png)\n",
    "\n",
    "`Adaptive crossover`: techniqu used in genetic algorithms to adjust the crossover rate (i.e. the probability of performin crossover) based on the performance of the current population. In traditional genetic algorithms the crossover rate is typically set to a fixed value. However in adaptive crossover the crossover rate is adjusted dynamically based on the current state of the population.\n",
    "- Use bit templates for crossover (1-first parent, 0-second parent). Learn which templates work best\n",
    "\n",
    "`Gray coding`: encode binary numbers in such a way that incrementing a number by 1 takes only 1 bit change (Sth like this: Order binary representations of numbers in such a way that the next number is only one bit changed: 0 - 1 - 11 - 110 - ...)\n",
    "\n",
    "- `Gaussian mutation`: mutate by adding a gaussian error to the mutation. Example: \n",
    "\n",
    "- `Lamarckian mutation`: search for locally best mutation\n",
    "    - for example if an individual has a high fitness score, we might increase the probability that it will pass on its traits to its offspring\n",
    "    - if an individual has a low fitness score, we migth decrease the probability that it will pass on its traits to its offspring\n",
    "\n",
    "![lamarckism](./images/lamarckism.png)\n",
    "\n",
    "- `Elitism`: chose n of the best solutions in generation and keep them for the next generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Describe the following evolutionary models: proportional and rank proportional roulette wheel, tournaments, single tournament, and stochastic universal sampling?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tournament selection`: involves running several \"tournaments\" among a few individuals chosen at random from the population. have agents \"battle\" each other, by assigning them probabilites according to their fitness values. Best solution $\\rightarrow$ best probability of winning. The winner of each tournament is selected for **crossover**.\n",
    "- if the tournament size `k` is larger, weak individuals have a smaler chance to be selected\n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "choose k (the tournament size) individuals from the population at random\n",
    "choose the best individual from the tournament with probability p\n",
    "choose the second best individual with probabilit p(1-p)\n",
    "choose the third best individual with probability p((1-p)^2)\n",
    "and so on\n",
    "```\n",
    "\n",
    "![tournament](./images/tournament-selection.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Proportional`: Assign each agent a probability according to their fitness value. Use randomly generated numbers to select agents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rank proportional`: assign each agent probabilty according to their **rank** of fitness value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Single tournament`: randomly split population into small groups and apply crossover to two best agents from each group, their offspring replace the two worst agents from the group."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stohastic universal sampling`: F = sum(all fitness values), N = size of population we want. Make a F/N interval. Assign part of the interval to each agent according to fitness values. Use RNG to generate numbers, if generated number is within an interval of some agent $\\rightarrow$ choose the agent.\n",
    "\n",
    "Pseudocode\n",
    "```\n",
    "SUS(Population, N)\n",
    "    F := total fitness of Population\n",
    "    N := number of offspring to keep\n",
    "    P := distance between the pointers (F/N)\n",
    "    Start := random number between 0 and P\n",
    "    Pointers := [Start + i*P | i in [0..(N-1)]]\n",
    "    return RWS(Population,Pointers)\n",
    "\n",
    "RWS(Population, Points)\n",
    "    Keep = []\n",
    "    for P in Points\n",
    "        I := 0\n",
    "        while fitness sum of Population[0..I] < P\n",
    "            I++\n",
    "        add Population[I] to Keep\n",
    "    return Keep\n",
    "```\n",
    "\n",
    "![sus](./images/sus.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Proportional Roulette wheel`: assign each agent a probability according to their fitness value. For example, if an individual has a fitness score that is twice as high as another individual's score, it will be twiece likely to be selected. To implement this model, we can create a roulette wheel, where each individual is represented by a slice of the wheel. The size of the slice is proportional to the individual's fitness score. We can then spin the wheel and select the individual showe slice pointer lands on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Rank proportional roulette wheel`: similar to the proportional roulette wheel, except the probability of an individual being selected is proportional to **its rank** rather than its fitness score. The highest-ranked individual will have the highest probability of being selected, and the lowest ranked individual will have the lowest probabilty of being selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How to prevent niche specialization in GA?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We punish agents that are too similar to others $\\rightarrow$ depending on the type of problem (min/max) decrease/increase the fitness value.\n",
    "2. Use a more diverse initial population\n",
    "3. More diverse genetic operators, such as different types of crossover and mutation\n",
    "4. Use a multi-objective optimization approach: rather than trying to optimize a single objective, using a multi-objective optimization approach can help prevent niche specialization by encouraging the gentic algorithm to find a set of solutions that are well-balanced across multiple objectives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Explain hyptheses on why Genetic Algorithms work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a big enough population and the right parameters, we can search a pretty big solution space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. What are the typical parameters of the GAs?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Population size\n",
    "1. Crossover rate\n",
    "1. Mutation rate\n",
    "1. Fitness function\n",
    "1. Selection mechanism\n",
    "1. Max number of iterations\n",
    "1. Termination criteria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Where to use GAs and where not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YES`:\n",
    "- fitness function easily defined, robustness, solution can be represented as a set of genes\n",
    "\n",
    "`NO`:\n",
    "- huge solution spaces with large solutions (e.g. list of lists of lists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Why are Genetic Algorithms suitable for multiobjective optimization, and what is Pareto optimal solution?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiobjective optimization**:\n",
    "- Use fitness functions with different objectives and try to improve them.\n",
    "\n",
    "**Pareto optimal solution**:\n",
    "- In a multiobjective optimization problem, a Pareto optimal solution is a solution that cannot be improved upon in one objecive without degrading another objective.\n",
    "\n",
    "Overall, GAs are a useful tool for multiobjective optimization because they are able to find a set of well-balanced solutions rather than just a single, globally optimal soution. This can be particularly useful in situations where it is not possible to optimize all objectives simultaneoulsy or where trade-offs between objectives must be made."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Explain the main problems of genetic programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs huge populations (thousands), it's slow, problems involving physical environments: making trees that are really executable, execution can change the environment which changes fitness function, calculating fitness function with simulation takes a lot of time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The success of genetic algorithms depends on the choice of evolutionary model. One of frequently used evolutionary models, **tournament selection**, uses tournaments of groups of agents to select candidates for reproduction. This method uses two parameters, the tournament size `t` and the selection probability `p`. Which of the below statements are true:\n",
    "    - [x] We typically conduct several tournaments and their winners take part in the reproduction.\n",
    "    - [ ] With an increasing selection probability we incrase chances of less successful canddiates.\n",
    "    - [ ] With an increasing tournament size we are approaching a random sampling of candidates.\n",
    "    - [x] With a decreasing tournament size we are approaching a random sampling of candidates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We applied a genetic algorithm to solve a problem. The current generation is composed of 5 individuals and is described in the table below:\n",
    "    | Individual | Fitness value |\n",
    "    | - | -  |\n",
    "    | 2 | 43 |\n",
    "    | 1 | 26 |\n",
    "    | 3 | 16 |\n",
    "    | 5 | 10 |\n",
    "    | 4 | 5  |\n",
    "\n",
    "    How many times will the individual 1  be sampled, if we are to select `N=10` individuals for recombination using the **stochastic universal sampling**. The individuals are ordered descendigly by fitness value and the random generator returned the value 0.2.\n",
    "    - F = 26 + 43 + 16 + 5 + 10 = 100\n",
    "    - N = 10\n",
    "    - P = F/N = 10\n",
    "    - Start = 0.2\n",
    "    - Pointers = [ 0.2, 10.2, 20.2, 30.2, 40.2, 50.2, 60.2, 70.2, 80.2, 90.2 ]\n",
    "    - Only 50.2 and 60.2 in interval [43, 43+26] -> two times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Which statements are true about genetic algorithms? (select one or more):\n",
    "   - [ ] genetic algorithms require a detailed description of how t oget a solution for the problem to be solved\n",
    "   - [ ] genetic algorithms systematically search the entire search space\n",
    "   - [x] genetic algorithms require an appropriate representation of individuals\n",
    "   - [ ] in each iteration of a genetic algorithm, the current population deterministically generates the next population\n",
    "   - [ ] if two successive generations are identical then the genetic algorithm has found the optimal solution\n",
    "   - [x] in general, there is no guarantee how good the solution found by a genetic algorithm will be\n",
    "   - [ ] a genetic algorithm will always find the optimal solution given enough time\n",
    "   - [x] genetic algorithms require a function to measure the quality of represented solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Try to estimate $f(X)$ so we can get the most accurate $Y$ to the actual result\n",
    "$$Y = f(X) + \\epsilon$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Describe the two main goals of ML, prediction and inference, and explain why they are sometimes in contradiction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Prediction`: process of using a model to make predictions about new, unseen data. \n",
    "\n",
    "`Inference`: we are interested in the type of relationship between Y and X, model interpretability is essential for inference. The goal of inference is to understand how the input data is related to the ouput.\n",
    "\n",
    "If we want a good accuracy (prediction), we might want a need a much more complicated model which will have a lower interpretability and vice versa. But it can also happen that some more complicated  models gives us bad results (overfitting)... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. What parametric and non-parametric ML methods exist?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parametric` methods (functional form of the relationship between X and Y):\n",
    "- linear regression\n",
    "- logistic regression\n",
    "- Neural networks\n",
    "\n",
    "`Non-parametric` methods:\n",
    "- knn\n",
    "- decision trees\n",
    "- random forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Describe the main characteristics of supervised, unsupervised and semi-supervised ML methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Supervised`: both X and Y are observed.\n",
    "\n",
    "`Unsupervised`: only X is observed. We need to use X to guess what Y would have been and build a model from there.\n",
    "\n",
    "`Semi-supervised`: combination of a small amount of labeled instances and a large amount of unlabelled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. What is the difference between Regression and Classification? Give examples of problems for each type. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Regression`: Y is continious\n",
    "- predicting the price of a house based on its size, location and other features\n",
    "\n",
    "`Classification`: Y is categorical (discrete output)\n",
    "- identifying the type of an object in an image (eg. dog, cat, car)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. What are association rules and how do they differ from from decision rules?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Association rules`: tell us how some event is associated with another, how some X is associated with Y. \n",
    "- Example rule `{onions, potatoes} -> {burger}`; if a customer buys onions and potatoes together, they are likely to also buy hambuerger meat.\n",
    "\n",
    "`Decision rules`: is a simple IF-THEN statement consisting of a condition and prediction.\n",
    "- if a patient has a high fever, then they likely have the flu\n",
    "- if it rains today and if it is april (condition), THEN it will ran tommorow (prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. What are outliers in ML?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Outliers` are those **data points** that are **significantly different** from rest of the dataset. It can be noise or an exception."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Contrast two different views on ML: as optimization and as a search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Optimization view`: usually the goal of classification is to find the best possible parameters of function to minimize the the error between prediceted output and true output. In this view, the model is seen as a fixed function, and the goal is to find the best parameters for that function by minimizing the error on the training data.\n",
    "\n",
    "1. `Search view`: the search view of ML is based on the idea that a model is a hypothesis space, and the goal is to search that space for a hypothesis that is consistent with the data. In this view, the process of training an ML model is seen as a search problem, where the goal is to find the best hypothesis by searching the space of possible hypotheses. The search can be done with different methods such such as `evolutionary algorithms`, `reinforcement learning` and Bayesian optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Describe the different properties of ML models: bias, variance, generalization, hypothesis language. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bias` error is an error from **erroneous assumptions** in the learning algorithmms.\n",
    "- **High bias** can cause an algorithm to miss the relevant relations between features and target outputs **underfitting**\n",
    "- **Low bias** will closely match the data set\n",
    "\n",
    "`Variance` is an error from sensitivity to small fluctations in the training set.\n",
    "- **High variance** may result from an algorithm modeling the random noise in the training data **overfitting**\n",
    "\n",
    "![underfitting-overfitting](./images/underfitting-overfitting.png)\n",
    "\n",
    "Usually, nonlinear algorithms have a lot of flexibility to fit the model, have high variance.\n",
    "\n",
    "![bias-vs-variance](./images/bias-vs-variance.png)\n",
    "\n",
    "\n",
    "`Generalization`: ability of a model to make accurate predictions on unseen data.\n",
    "\n",
    "`Hypothesis language` describes the hypotheses which machine learning system outpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. What is the bias-variance trade-off in ML?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have too much bias, we won't have a lot of variance giving us a very inflexible method that doesn't predict well. If we have too much variance, the model could overfit to the training adta and will not work well with new unseen data. In both cases the error of prediction will be high, so we want to find the right balance between bias and variance.\n",
    "\n",
    "1. **Low-Bias, Low-variance**: the combination of low bias and low variance shows an ideal machine learning model. However, it is not possible practically.\n",
    "2. **Low-Bias, High-variance**: with low bias and high variance, model predictions are inconsistent and accurate on average. This case occurs when the model learns with a large number of parameters and hence leads to an **overfitting**.\n",
    "3. **High-Bias, Low-variance**: With high bias and low variance, predictions are consistent but inaccurate on average. This case occurs when a model does not learn well with the training dataset or uses few numbers of the parameter. It leads to **Underfitting** problems in the model.\n",
    "4. **High-bias**, **High-variance**: with high bias and high variance predictions are inconsistent and also inaccurate on average.\n",
    "\n",
    "![bias-variance-tradeoff](./images/bias-variance-tradeoff.png)\n",
    "\n",
    "While building the machine learning model, it is really important to take care of bias and variance in order to avoid overfitting and underfitting the model. If the model is very simple with **fewer parameters**, it may have low variance and high bias. Whereas, if the model has a **large** number of **parameters**, it will have high variance and low bias. So it is required to make a balance between bias and variance errors, and this balance between the bias error is known as the **Bias-Variance trade-off**.\n",
    "\n",
    "![bias-variance-trade-off](./images/bias-variance-trade-off.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Describe the double descent concerning bias-variance trade off"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of data points is large enough, increasing the model complexity **may** actually improve performance even after the optimal point, due to a reduction in bias. The results in a \"double descent\" curve which has a second minimum point, where increasing the model complexity will again improve performance.\n",
    "\n",
    "The double descent phenomenon challenges the traditional bias-variance trade-off as it suggests that increasing the model complexity may improve performance even after the point where bias and variance are optimally balanced, when the number of data poins is large enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Describe bias-variance trade-off in relation to kNN classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance generally decreases with increasing k, bias increases with increasing k."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Describe methods that can speed-up the kNN algorithm: k-d trees, R-trees, RKD-tree, locally sensitive hashing, and hiearchical k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. What are the Bayes error rate and Bayes optimal classifier?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bayes error rate` is the lowest possible error rate that can be achieved by any classifier for a given problem, assuming that the classifier has access to the true probability distributions of the data.\n",
    "\n",
    "`Bayes optimal classifier` is a probabilistic model that makes the most probable prediction for a new example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. Describe properties of the following models: kNN, decision rules, bagging, boosting, random forests, stacking, AODE, MARS, SVM, neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kNN`: majority class of k nearest neighbors\n",
    "\n",
    "`Decision rules` is a function which maps an observation to an appropriate action.\n",
    "\n",
    "`Bagging=Bootstrap aggregation`: **ensemble method**: majority vote; multiple \"bags\" with random samples of data of training set is selected with replacement - meaning that the individual data points can be chosen more than once. Weak learners are trained in parallel on each bag and the final prediction is the majority vote of all the weak learners.\n",
    "- **high variance** and **low bias**\n",
    "- underlying model can be anything\n",
    "\n",
    "![bagging](./images/bagging.png)\n",
    "\n",
    "`Boosting`: **ensemble method**; weak learners are trained sequentially, each trying to correct its predecessor. The final prediction is the weighted sum of all the weak learners. (`XGBoost`, `AdaBoost`)\n",
    "- used when **low variance and high bias is observed**\n",
    "\n",
    "![bagging-vs-boosting](./images/bagging-vs-boosting.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. What is the differenece between training and testing error? Why do we need an evaluation set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Training error` is the error rate we get on training data\n",
    "\n",
    "`Testing error` is the error we get on the test data.\n",
    "\n",
    "Mostly if the training error is very low, the model will overfit, which will produce a high testing error and a badly generalized model.\n",
    "We need the evaluation set to test our model on previously unseen data and see if we overfitted it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Describe the properties and purpose of evaluation with cross-validation. Describe different biases of ML models stemming from data: reporting bias, automation bias, selection bias, group attribution bias, implicit bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select correct statements about bias-variance.\n",
    "    - [ ] For decision tree models, the bias is realively high, but variance is low\n",
    "        - decision tree models have **low bias** because they are able to capture complex and non-linear relationships in the data, by creating multiple decision rules based on the input features. A decision tree is able to recursively split the data into smaller and smaller subsets, which allows it to learn more patterns in the data. This is the main beenfit of using decision trees, they can adapt to more complex problems\n",
    "        - **high variance**: they are prone to overfitting. A decision tree can be grown to fit the noise in the data as well as the underlying pattern\n",
    "    - [x] for kNN classifier, the variance is generally decreasing with increasing `k`\n",
    "    - [x] for kNN classifier the bias is generally increasing with increasing `k`\n",
    "    - [ ] for kNN classifier the bias is generally decreasing with increasing `k`\n",
    "    - [ ] for linear regression models, the bias is relatively low, but the variance is high.\n",
    "    - [ ] for kNN classifier, the bias-variance trade-off  does not apply as we can select optimal k.\n",
    "    - [x] for linear regression models, the bias is relatively high, but the vairance is low"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We trained a binary classifier using a dataset consisting of 1000 examples (900 from the class \"A\" and 100 from the class \"B\"). We used an independent testing set to estimate the quality of our model and achieved 88% classification accuracy. What can we conclude about the quality of the trained model based only on the available information?\n",
    "    - [x] the model is useless, as it is worse than the majority classifier\n",
    "    - [ ] we cannost say anything about the quality of the model\n",
    "    - [ ] the model is useful because it is much better than a random guess\n",
    "    - [ ] the model is useful if it is comprehensible\n",
    "    - [ ] the model is useful as its accurcay is in accordance with the class distribution in our training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eb4c12fa1609752104d9388b2ce0ed54a404f0e7364c7bcf49ffa5d2d1d99e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
